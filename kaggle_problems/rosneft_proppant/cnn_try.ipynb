{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "while not os.getcwd().endswith('ml'):\n",
    "    os.chdir('..')\n",
    "sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from shutil import copyfile\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from ml_helpers.image_helpers import display_images\n",
    "from ml_helpers.common_helpers import display_history_metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "from kaggle_problems.rosneft_proppant.workspace.common import bins\n",
    "\n",
    "from kaggle_problems.rosneft_proppant.workspace.common import TARGET_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"kaggle_problems/rosneft_proppant/data/\"\n",
    "MODEL_DIR = \"kaggle_problems/rosneft_proppant/worace/models\"\n",
    "GENERATED_DIR = \"kaggle_problems/rosneft_proppant/data/generated/\"\n",
    "GENERATED_IMG_DIR = GENERATED_DIR + \"colored_img\"\n",
    "GENERATED_LABELS_DIR = GENERATED_DIR + \"labels\"\n",
    "DF_RATE = 1.\n",
    "\n",
    "sources = ['bw'] #'colored']\n",
    "source_to_fraction = {\n",
    "    'bw': 'bw',\n",
    "    'colored': 'colored',\n",
    "    'threshed': 'bw'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_fraction(train):\n",
    "    for fraction in source_to_fraction.values():\n",
    "        img_numbers = [int(img[0:-len(\".jpg\")]) for img in os.listdir(DATA_DIR + fraction + \"_main_area\") if img.endswith('.jpg')]\n",
    "        train.loc[train.ImageId.isin(img_numbers), 'fraction'] = fraction\n",
    "    return train\n",
    "\n",
    "def get_fraction_sievs(data, fraction):\n",
    "    data_fraction = data[data.fraction == fraction]\n",
    "    result_bins = []\n",
    "    for b in bins:\n",
    "        if data_fraction[b].sum() > 1e-5:\n",
    "            result_bins.append(b)\n",
    "    return result_bins\n",
    "        \n",
    "\n",
    "def common_df_processing(data):\n",
    "    for i in bins:\n",
    "        if i in data.columns:\n",
    "            data = data[~data[i].isnull()]\n",
    "\n",
    "    data[\"filename\"] = data['ImageId'].astype(str) + '.jpg'\n",
    "    return data\n",
    "\n",
    "def get_validation(source):\n",
    "    validation = pd.read_csv(\"{}labels/train.csv\".format(DATA_DIR))\n",
    "    \n",
    "    validation.fraction = None\n",
    "    validation = enrich_fraction(validation)\n",
    "\n",
    "    validation = validation[~validation.fraction.isnull()]\n",
    "    \n",
    "    fraction = source_to_fraction[source]\n",
    "    validation = validation[validation['fraction'] == source_to_fraction[source]]\n",
    "\n",
    "    validation = common_df_processing(validation)\n",
    "    return validation\n",
    "\n",
    "def get_train(source):\n",
    "    train = pd.read_csv(\"{}/generated_{}_train.csv\".format(GENERATED_LABELS_DIR, source))\n",
    "    \n",
    "    train = common_df_processing(train)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_sievs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinsExtraction(Model):\n",
    "    def __init__(self, fraction):\n",
    "        super(BinsExtraction, self).__init__()\n",
    "        self.FilterSize1 = 32\n",
    "        self.FilterSize2 = 16\n",
    "        self.FilterSize3 = 8\n",
    "        \n",
    "        self.model_layers = [\n",
    "            tf.keras.layers.Conv2D(filters=self.FilterSize1, kernel_size=(5, 5), strides=(5, 5)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Dropout(rate=0.5),\n",
    "\n",
    "            tf.keras.layers.Conv2D(filters=self.FilterSize2, kernel_size=(3, 3), strides=(3, 3)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Dropout(rate=0.5),\n",
    "            \n",
    "#             tf.keras.layers.Conv2D(filters=self.FilterSize3, kernel_size=(3, 3), strides=(3, 3)),\n",
    "#             tf.keras.layers.BatchNormalization(),\n",
    "#             tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "#             tf.keras.layers.Dropout(rate=0.5),\n",
    "\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(rate=0.5),\n",
    "            tf.keras.layers.Dense(len(fraction_sievs[fraction]), activation='softmax'),\n",
    "        ]\n",
    "\n",
    "    def call(self, x, *args, **kwargs):\n",
    "        for model_layer in self.model_layers:\n",
    "            x = model_layer(x, *args, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Input Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_val_datagen(train, source, train_size=0.8):\n",
    "#     fraction = source_to_fraction[source]\n",
    "#     train_fraction = train[train['fraction'] == source_to_fraction[source]]\n",
    "    \n",
    "#     train_fraction, val_fraction = train_test_split(train_fraction, train_size=train_size, random_state=123)\n",
    "    \n",
    "#     bins_fraction = fraction_sievs[fraction]\n",
    "    \n",
    "#     datagen = ImageDataGenerator()\n",
    "\n",
    "#     train_generator = datagen.flow_from_dataframe(\n",
    "#             train_fraction.sample(n=int(len(train_fraction) * DF_RATE)),\n",
    "#             directory=\"kaggle_problems/rosneft_proppant/data/{}_main_area\".format(source),\n",
    "#             x_col='filename', \n",
    "#             y_col=bins_fraction,\n",
    "#             target_size=TARGET_SHAPE,\n",
    "#             batch_size=16,\n",
    "#             class_mode='other',\n",
    "#     )\n",
    "    \n",
    "#     val_generator = datagen.flow_from_dataframe(\n",
    "#         val_fraction.sample(n=int(len(val_fraction) * DF_RATE)),\n",
    "#         directory=\"kaggle_problems/rosneft_proppant/data/{}_main_area\".format(source),\n",
    "#         x_col='filename', \n",
    "#         y_col=bins_fraction,\n",
    "#         target_size=TARGET_SHAPE,\n",
    "#         batch_size=16,\n",
    "#         class_mode='other')\n",
    "#     return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_datagen(train, validation, source):\n",
    "    fraction = source_to_fraction[source]\n",
    "    bins_fraction = fraction_sievs[fraction]\n",
    "    \n",
    "    datagen = ImageDataGenerator()\n",
    "\n",
    "    train_generator = datagen.flow_from_dataframe(\n",
    "            train.sample(n=int(len(train) * DF_RATE)),\n",
    "            directory=\"kaggle_problems/rosneft_proppant/data/generated/{}_img\".format(source),\n",
    "            x_col='filename', \n",
    "            y_col=bins_fraction,\n",
    "            target_size=TARGET_SHAPE,\n",
    "            batch_size=16,\n",
    "            class_mode='other',\n",
    "    )\n",
    "    \n",
    "    validation_generator = datagen.flow_from_dataframe(\n",
    "            validation.sample(n=int(len(validation) * DF_RATE)),\n",
    "            directory=\"kaggle_problems/rosneft_proppant/data/{}_main_area\".format(source),\n",
    "            x_col='filename', \n",
    "            y_col=bins_fraction,\n",
    "            target_size=TARGET_SHAPE,\n",
    "            batch_size=16,\n",
    "            class_mode='other',\n",
    "    )\n",
    "    \n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input generator checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, labels = get_train_val_datagen(train, validation, 'colored')[0].next()\n",
    "# display_images(img[0:8].astype(int), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "earlystop = EarlyStopping(patience=10)\n",
    "\n",
    "callbacks = [earlystop, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(true, predicted):\n",
    "    return tf.keras.backend.mean(tf.math.reduce_sum((true - predicted) ** 2 / (true + predicted), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 validated image filenames.\n",
      "Found 615 validated image filenames.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1 steps, validate for 39 steps\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 194s 194s/step - loss: 1.0455 - val_loss: nan\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 [==============================] - 195s 195s/step - loss: 0.8621 - val_loss: nan\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 190s 190s/step - loss: 0.8884 - val_loss: nan\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1/1 [==============================] - 190s 190s/step - loss: 0.9365 - val_loss: nan\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 197s 197s/step - loss: 0.8514 - val_loss: nan\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1/1 [==============================] - 190s 190s/step - loss: 0.9014 - val_loss: nan\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 190s 190s/step - loss: 0.9127 - val_loss: nan\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1/1 [==============================] - 191s 191s/step - loss: 0.8990 - val_loss: nan\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 190s 190s/step - loss: 0.8767 - val_loss: nan\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1/1 [==============================] - 191s 191s/step - loss: 0.8265 - val_loss: nan\n",
      "WARNING:tensorflow:From /Users/skrrydg/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/skrrydg/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: kaggle_problems/rosneft_proppant/worace/models/model_benchmark_bw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: kaggle_problems/rosneft_proppant/worace/models/model_benchmark_bw/assets\n"
     ]
    }
   ],
   "source": [
    "for source, i in zip(sources, range(len(sources))):\n",
    "    validation = get_validation(source)\n",
    "    train = get_train(source)\n",
    "    fraction = source_to_fraction[source]\n",
    "    \n",
    "    fraction_sievs[fraction] = get_fraction_sievs(validation, fraction)\n",
    "    \n",
    "    model = BinsExtraction(fraction)\n",
    "    model.compile(\n",
    "        loss=metric,\n",
    "        optimizer='rmsprop',\n",
    "    )\n",
    "\n",
    "    train_datagen, val_datagen = get_train_val_datagen(train, validation, source)\n",
    "\n",
    "    history = model.fit(\n",
    "        x=train_datagen,\n",
    "        epochs=50,\n",
    "        validation_data=val_datagen,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    Path(MODEL_DIR).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    with open(MODEL_DIR + \"/history_model_benchmark_{}.pickle\".format(source), 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        \n",
    "    model.save(MODEL_DIR + \"/model_benchmark_{}\".format(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins_metric(predicted, true):\n",
    "    return 0.5 * np.sum((predicted - true) ** 2 / (predicted + true)) / predicted.shape[0]\n",
    "\n",
    "def get_bins_metric_by_image(predicted, true):\n",
    "    return np.sum(0.5 * (predicted - true) ** 2 / (predicted + true), axis=1)\n",
    "\n",
    "def get_bins_metric_by_bins(predicted, true):\n",
    "    return np.sum(0.5 * (predicted - true) ** 2 / (predicted + true), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Total bin loss: {}\".format(get_bins_metric(predicted_labels, all_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colored----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kaggle_problems/rosneft_proppant/worace/models/history_model_benchmark_colored.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9d013149d7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mget_custom_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/history_model_benchmark_{}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kaggle_problems/rosneft_proppant/worace/models/history_model_benchmark_colored.pickle'"
     ]
    }
   ],
   "source": [
    "# for source, i in zip(sources, range(len(sources))):\n",
    "#     fraction = source_to_fraction[source]\n",
    "#     print(source + \"-\" * 100)\n",
    "#     from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "#     get_custom_objects().update({'metric': metric})\n",
    "\n",
    "#     with open(MODEL_DIR + \"/history_model_benchmark_{}.pickle\".format(source), 'rb') as f:\n",
    "#         history = pickle.load(f)\n",
    "\n",
    "#     model = tf.keras.models.load_model(MODEL_DIR + \"/model_benchmark_{}\".format(source), \n",
    "#                                        compile=False)\n",
    "#     model.compile(\n",
    "#         loss=metric,\n",
    "#         optimizer='rmsprop',\n",
    "#        # metrics=['mse']\n",
    "#     )                                                                               \n",
    "\n",
    "#     display_history_metrics(history, source)\n",
    "#     print(source + '-' * 100)\n",
    "    \n",
    "#     train_datagen = get_train_val_datagen(train, fraction)[0]\n",
    "\n",
    "#     predicted_labels = []\n",
    "#     all_labels = []\n",
    "#     train_fraction = train[train['fraction'] == fraction]\n",
    "\n",
    "#     for i in range(int(train_fraction.shape[0]) // 16):\n",
    "#         imgs, labels = train_datagen.next()\n",
    "#         predicted_labels.extend(model.predict(imgs))\n",
    "#         all_labels.extend(labels)\n",
    "#     predicted_labels = np.array(predicted_labels)\n",
    "#     all_labels = np.array(all_labels)\n",
    "\n",
    "#     losses_by_img = get_bins_metric_by_image(predicted_labels, all_labels)\n",
    "#     plt.hist(losses_by_img, bins=100)\n",
    "#     plt.show()\n",
    "\n",
    "#     losses_by_bins = get_bins_metric_by_bins(predicted_labels, all_labels)\n",
    "#     plt.hist(losses_by_bins, bins=100)\n",
    "#     plt.show()\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook kaggle_problems/rosneft_proppant/cnn_try.ipynb to script\n",
      "[NbConvertApp] Writing 10260 bytes to kaggle_problems/rosneft_proppant/cnn_try.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script kaggle_problems/rosneft_proppant/cnn_try.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
